% ========== 第二章：算法实现与数据分析 ==========
\documentclass[../main]{subfiles}
\begin{document}

\section{优化算法实现}

\subsection{梯度下降算法}

梯度下降是求解无约束优化问题的经典方法\autocite{nocedal2006numerical}。

\begin{algorithm}[H]
\caption{梯度下降算法}
\label{alg:gradient_descent}
\KwIn{目标函数 $f(x)$，初始点 $x_0$，学习率 $\alpha$，容忍误差 $\epsilon$}
\KwOut{最优解 $x^*$}

\Fn{\FMain{$f, x_0, \alpha, \epsilon$}}{
    $x \leftarrow x_0$\;
    $k \leftarrow 0$\;
    \Repeat{$\|\nabla f(x)\| < \epsilon$}{
        $g \leftarrow \nabla f(x)$\;
        $x \leftarrow x - \alpha \cdot g$\;
        $k \leftarrow k + 1$\;
    }
    \Return $x$\;
}
\end{algorithm}

\subsection{遗传算法流程}

遗传算法是一种基于自然选择和遗传机制的全局优化算法\autocite{holland1992genetic,goldberg1989genetic}。

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=2cm]
    \tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=red!30]
    \tikzstyle{process} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=orange!30]
    \tikzstyle{decision} = [diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30]
    \tikzstyle{arrow} = [thick,->,>=stealth]

    \node (start) [startstop] {初始化种群};
    \node (evaluate) [process, below of=start] {适应度评估};
    \node (decide) [decision, below of=evaluate, yshift=-0.5cm] {满足终止条件？};
    \node (selection) [process, below of=decide, yshift=-0.5cm] {选择操作};
    \node (crossover) [process, right of=selection, xshift=2cm] {交叉操作};
    \node (mutation) [process, above of=crossover] {变异操作};
    \node (stop) [startstop, right of=decide, xshift=4cm] {输出最优解};

    \draw [arrow] (start) -- (evaluate);
    \draw [arrow] (evaluate) -- (decide);
    \draw [arrow] (decide) -- node[anchor=east] {否} (selection);
    \draw [arrow] (decide) -- node[anchor=south] {是} (stop);
    \draw [arrow] (selection) -- (crossover);
    \draw [arrow] (crossover) -- (mutation);
    \draw [arrow] (mutation) |- (evaluate);
\end{tikzpicture}
\caption{遗传算法流程图}
\label{fig:genetic_algorithm}
\end{figure}

除了遗传算法外，粒子群优化算法也是一种有效的群体智能优化方法\autocite{kennedy1995particle}。

\section{数据分析与可视化}

\subsection{函数优化过程可视化}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={目标函数与优化轨迹},
    xlabel={$x_1$},
    ylabel={$x_2$},
    xmin=-3, xmax=3,
    ymin=-3, ymax=3,
    grid=major,
    legend pos=north east
]
    % 简化的等高线（手动绘制圆形等高线）
    \addplot[domain=0:360, samples=100, color=gray!50] ({cos(x)}, {sin(x)});
    \addplot[domain=0:360, samples=100, color=gray!50] ({1.5*cos(x)}, {1.5*sin(x)});
    \addplot[domain=0:360, samples=100, color=gray!50] ({2*cos(x)}, {2*sin(x)});
    \addplot[domain=0:360, samples=100, color=gray!50] ({2.5*cos(x)}, {2.5*sin(x)});
    
    % 优化轨迹
    \addplot[color=red, mark=*, mark size=2pt, thick] coordinates {
        (-2,-2) (-1.5,-1.5) (-1,-1) (-0.5,-0.5) (0,0)
    };
    
    \legend{等高线, 优化轨迹}
\end{axis}
\end{tikzpicture}
\caption{二次函数的梯度下降优化轨迹}
\label{fig:optimization_path}
\end{figure}

\subsection{算法性能对比}

不同优化算法的收敛性能存在显著差异，如\cref{fig:algorithm_comparison}所示。

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={不同算法收敛性能对比},
    xlabel={迭代次数},
    ylabel={目标函数值},
    xmin=0, xmax=100,
    ymin=0, ymax=10,
    grid=major,
    legend pos=north east
]
    % 梯度下降
    \addplot[color=blue, mark=square, mark size=1pt] coordinates {
        (0,10) (10,5) (20,2.5) (30,1.25) (40,0.625) (50,0.31) (60,0.16) (70,0.08) (80,0.04) (90,0.02) (100,0.01)
    };
    
    % 牛顿法
    \addplot[color=red, mark=circle, mark size=1pt] coordinates {
        (0,10) (5,2) (10,0.1) (15,0.001) (20,0.0001)
    };
    
    % 遗传算法
    \addplot[color=green, mark=triangle, mark size=1pt] coordinates {
        (0,10) (10,8) (20,6) (30,4) (40,2.5) (50,1.5) (60,0.8) (70,0.4) (80,0.2) (90,0.1) (100,0.05)
    };
    
    \legend{梯度下降, 牛顿法, 遗传算法}
\end{axis}
\end{tikzpicture}
\caption{算法收敛速度对比}
\label{fig:algorithm_comparison}
\end{figure}

\subsection{实验数据统计分析}

实验结果的统计分析对于验证模型的有效性至关重要。

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={模型预测精度分析},
    xlabel={测试样本编号},
    ylabel={预测误差 (\%)},
    xmin=0, xmax=20,
    ymin=0, ymax=15,
    grid=major,
    legend pos=north east
]
    \addplot[color=blue, mark=square, error bars/.cd, y dir=both, y explicit] 
    coordinates {
        (1,5) +- (0,1.2)
        (2,3.5) +- (0,0.8)
        (3,7.2) +- (0,1.5)
        (4,4.1) +- (0,0.9)
        (5,6.8) +- (0,1.3)
        (6,2.9) +- (0,0.7)
        (7,8.5) +- (0,1.8)
        (8,3.2) +- (0,0.6)
        (9,5.7) +- (0,1.1)
        (10,4.6) +- (0,0.8)
        (11,7.1) +- (0,1.4)
        (12,3.8) +- (0,0.9)
        (13,6.3) +- (0,1.2)
        (14,4.9) +- (0,1.0)
        (15,5.4) +- (0,1.1)
        (16,3.6) +- (0,0.7)
        (17,7.8) +- (0,1.6)
        (18,4.3) +- (0,0.8)
        (19,6.1) +- (0,1.3)
        (20,5.2) +- (0,1.0)
    };
    
    \addplot[color=red, dashed, thick] coordinates {(0,5.5) (20,5.5)};
    
    \legend{预测误差, 平均误差}
\end{axis}
\end{tikzpicture}
\caption{模型预测误差分布}
\label{fig:prediction_error}
\end{figure}

从\cref{fig:prediction_error}可以看出，模型的平均预测误差约为5.5\%，这表明所建立的数学模型具有较好的预测精度。

\end{document} 